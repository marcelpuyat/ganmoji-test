* Generator loss is optimizing number of fake images classified as 1 (non saturating loss function)
* LeakyRelu for discriminator to avoid sparse gradients
* Feature Matching
* Instance noise in discriminator
* Compute minibatch discrim at beginning, but append to the end (tested it out, did not do, not practical for production (needs batch))
* WGAN-GP penalties as suggested by new Goodfellow paper (Oct 2017)
* https://openreview.net/pdf?id=HJKkY35le for mode collapse issues

Things I tried that didn't make much of a difference:
- Leaky ReLU for generator
- Running generator twice as often
- Higher learning rate for generator
- Dropout in discriminator
- Label smoothing for discriminator (instance noise was better)
- DRAGAN gradient penalty